{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"자연어처리기말프로젝트(프렌즈).ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"mount_file_id":"1nWc96YduTMmS300u5yBCtEiSCS9KWLtp","authorship_tag":"ABX9TyM1dwYhOeWTMRJb4HdfhG4I"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"X4lrwiXxt0yG"},"source":["\r\n","**<h1>프렌즈 영화 스크립트 코퍼스 분석</h1>**\r\n","\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"lxnScKFtjWew"},"source":["import json \r\n","import pandas as pd\r\n","import numpy as np\r\n","%matplotlib inline\r\n","import matplotlib.pyplot as plt\r\n","import re\r\n","import urllib.request\r\n","from tensorflow.keras.preprocessing.text import Tokenizer\r\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NTI13vIEt_VC"},"source":["데이터 로드"]},{"cell_type":"code","metadata":{"id":"BvF9-Tq-ipjj"},"source":["!tar -zxvf /content/drive/MyDrive/고려대빅데이터융합학과/빅데이터처리를위한자연어처리/KU-NLP-2020-1/EmotionLines_friends_annotation.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JO7i8Um4uGnw"},"source":["json 데이터 전처리 작업\r\n"]},{"cell_type":"code","metadata":{"id":"yE49kNUMjXiu"},"source":["a = pd.read_json(r'/content/EmotionLines/Friends/friends_train.json')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ClpI4kunPlCg"},"source":["a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CsIIUQaah0mj"},"source":["doc_type_list = [k for k in a.keys()]\r\n","doc_list =[j for j in a[0].keys()]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b1d-s1qhjYG-"},"source":["dict_es =pd.DataFrame()\r\n","dict_es1=pd.DataFrame(columns=['speaker','utterance','emotion','annotation'])\r\n","dict_es2=pd.DataFrame(columns=['speaker','utterance','emotion','annotation'])\r\n","for j in doc_list:\r\n","    for k in doc_type_list:\r\n","           try:\r\n","                dict_es[k] =pd.Series(list(a[k][j].values()),index =['speaker','utterance','emotion','annotation'])\r\n","                print(dict_es[k])\r\n","                dict_es1 = dict_es1.append(dict_es[k],ignore_index=True)\r\n","                print(dict_es1)\r\n","           except:\r\n","                pass\r\n","    \r\n","    k=0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6K3d3GW9lGTu"},"source":["dict_es1#한개의 리스트로 정렬"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P_E38AnEuPyA"},"source":["데이터 탐색\r\n"]},{"cell_type":"code","metadata":{"id":"i6Ck4VCbiVE8"},"source":["dict_es1['emotion'].value_counts().plot(kind = 'bar')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f6n1OtI2jEmW"},"source":["print(dict_es1.groupby('emotion').size().reset_index(name = 'count'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Acw9H7L4kVzx"},"source":["train_data =dict_es1[['utterance','emotion']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ZBSZVQqeFr9"},"source":["print('훈련용 스크립트 개수 :',len(train_data))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rcnk8fdjuTz3"},"source":["데이터 전처리\r\n","<p>1.공백 데이터 처리\r\n","<p>2.다른 언어 및 특수문자 처리\r\n","<p>3.불용어 처리\r\n","<p>4.빈도수가 낮은 단어 처리\r\n","<p>5.토큰작업 및 벡터화 처리"]},{"cell_type":"code","metadata":{"id":"CtfLNAIGjYJ-"},"source":["train_data.drop_duplicates(subset=['utterance'], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I67WTUd_gy9a"},"source":["train_data.loc[train_data.utterance.isnull()]#null check"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XbD4BitnLulf"},"source":["train_data = train_data.dropna(how = 'any')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E8Ga55AZjYM2"},"source":["train_data['utterance'] = train_data['utterance'].str.replace(\"[^a-zA-Z ]\",\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-g5ZoFX6jYP2"},"source":["train_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"85UH0QLJjYSw"},"source":["train_data['utterance'].replace('', np.nan, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ms04LRJbJfu6"},"source":["import nltk\r\n","nltk.download('stopwords')\r\n","nltk.download('punkt')\r\n","from nltk.corpus import stopwords\r\n","\r\n","\r\n","stopwordss =set(stopwords.words('english'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YC6qC7NmjYVm"},"source":["X_train = []\r\n","for sentence in train_data['utterance']:\r\n","    temp_X = []\r\n","    temp_X = nltk.word_tokenize(sentence) # 토큰화\r\n","    temp_X = [word for word in temp_X if not word in stopwordss] # 불용어 제거\r\n","    X_train.append(temp_X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1It0oFMcjYYn"},"source":["X_train[:3]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Brh6lLerjYbW"},"source":["tokenizer = Tokenizer()\r\n","tokenizer.fit_on_texts(X_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yEmmPDk5jYeO"},"source":["print(tokenizer.word_index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pb685-bijYhZ"},"source":["threshold = 3\r\n","total_cnt = len(tokenizer.word_index) # 단어의 수\r\n","rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\r\n","total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\r\n","rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\r\n","\r\n","# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\r\n","for key, value in tokenizer.word_counts.items():\r\n","    total_freq = total_freq + value\r\n","\r\n","    # 단어의 등장 빈도수가 threshold보다 작으면\r\n","    if(value < threshold):\r\n","        rare_cnt = rare_cnt + 1\r\n","        rare_freq = rare_freq + value\r\n","\r\n","print('단어 집합(vocabulary)의 크기 :',total_cnt)\r\n","print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\r\n","print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\r\n","print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BBY35x5xjYkR"},"source":["# 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거.\r\n","# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2\r\n","vocab_size = total_cnt - rare_cnt + 2\r\n","print('단어 집합의 크기 :',vocab_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oJLlc2L8OLu8"},"source":["tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') \r\n","tokenizer.fit_on_texts(X_train)\r\n","X_train = tokenizer.texts_to_sequences(X_train)\r\n","#X_test = tokenizer.texts_to_sequences(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZGHLMyWpOTt7"},"source":["X_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oboiphdLOx_-"},"source":["y_train = np.array(train_data['emotion'])\r\n","#y_test = np.array(test_data['label'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WtFgmAHTOyh6"},"source":["y_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gf3-aawnOykt"},"source":["drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4vwC1u7MOynS"},"source":["# 빈 샘플들을 제거\r\n","X_train = np.delete(X_train, drop_train, axis=0)\r\n","y_train = np.delete(y_train, drop_train, axis=0)\r\n","print(len(X_train))\r\n","print(len(y_train))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eGSQiywmOyqK"},"source":["print('스크립트의 최대 길이 :',max(len(l) for l in X_train))\r\n","print('스크립트의 평균 길이 :',sum(map(len, X_train))/len(X_train))\r\n","plt.hist([len(s) for s in X_train], bins=50)\r\n","plt.xlabel('length of samples')\r\n","plt.ylabel('number of samples')\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PfXUftfoOys6"},"source":["def below_threshold_len(max_len, nested_list):\r\n","  cnt = 0\r\n","  for s in nested_list:\r\n","    if(len(s) <= max_len):\r\n","        cnt = cnt + 1\r\n","  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pVOk0jK8Oyvy"},"source":["max_len = 30\r\n","below_threshold_len(max_len, X_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"16Kz-KioOyyt"},"source":["X_train = pad_sequences(X_train, maxlen = max_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n1uWo7XwPOtP"},"source":["from sklearn.preprocessing import LabelEncoder\r\n","\r\n","le = LabelEncoder()\r\n","y_train1 = le.fit_transform(y_train1)\r\n","y_test1 = le.fit_transform(y_test1)\r\n","#le.inverse_transform(result)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wv08wb5rPtA3"},"source":["from sklearn.model_selection import train_test_split\r\n","X_train1, X_test1, y_train1, y_test1 = train_test_split(X_train, y_train, test_size= 0.1, random_state=1234)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2rK36XJKxvcU"},"source":["모델 학습"]},{"cell_type":"code","metadata":{"id":"iyKDVAJKQliC"},"source":["from tensorflow.keras.models import Sequential, Model\r\n","from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense, Input, Flatten, Concatenate\r\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\r\n","from tensorflow.keras.models import load_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hs6dfnvxQmwo"},"source":["embedding_dim = 128\r\n","dropout_prob = (0.5, 0.8)\r\n","num_filters = 128"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vPhAk1iFQol2"},"source":["model_input = Input(shape = (max_len,))\r\n","z = Embedding(vocab_size, embedding_dim, input_length = max_len, name=\"embedding\")(model_input)\r\n","z = Dropout(dropout_prob[0])(z)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nrGU0YgGQrlh"},"source":["conv_blocks = []\r\n","\r\n","for sz in [3, 4, 5]:\r\n","    conv = Conv1D(filters = num_filters,\r\n","                         kernel_size = sz,\r\n","                         padding = \"valid\",\r\n","                         activation = \"relu\",\r\n","                         strides = 1)(z)\r\n","    conv = GlobalMaxPooling1D()(conv)\r\n","    conv = Flatten()(conv)\r\n","    conv_blocks.append(conv)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WQPpGmYaQuy7"},"source":["z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\r\n","z = Dropout(dropout_prob[1])(z)\r\n","z = Dense(128, activation=\"relu\")(z)\r\n","#z = Dropout(dropout_prob[1])(z)\r\n","#z = Dense(128, activation=\"relu\")(z)\r\n","model_output = Dense(8, activation=\"softmax\")(z)\r\n","\r\n","model = Model(model_input, model_output)\r\n","model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_jYC-nw8Rf4H"},"source":["es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\r\n","mc = ModelCheckpoint('CNN_model2.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\r\n","\r\n","model.fit(X_train1, y_train1, batch_size = 64, epochs=10, validation_data = (X_test1, y_test1), verbose=2, callbacks=[es, mc])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rsv1_xroTv0f"},"source":["loaded_model = load_model('CNN_model2.h5')#model2 -> 45.51% , model1->45.85%\r\n","print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test1, y_test1)[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PjxjeeFDx9FL"},"source":["테스트 데이터 예측 작업"]},{"cell_type":"code","metadata":{"id":"ddG0cL_fQHOd"},"source":["data_final = pd.read_csv('/content/drive/MyDrive/고려대빅데이터융합학과/빅데이터처리를위한자연어처리/en_data.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EhI0-64HBeK9"},"source":["data_final['utterance'] = data_final['utterance'].str.replace(\"[^a-zA-Z ]\",\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hEQ-8219PzCx"},"source":["def sentiment_predict(new_sentence):\r\n","  new_sentence = nltk.word_tokenize(new_sentence) # 토큰화\r\n","  new_sentence = [word for word in new_sentence if not word in stopwordss] # 불용어 제거\r\n","  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\r\n","  pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\r\n","  score = loaded_model.predict(pad_new) # 예측\r\n","  y_classes = score.argmax(axis=-1)\r\n","  return y_classes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mnxq-nrlM5Vl"},"source":["data_final['Predicted'] = data_final['utterance'].apply(sentiment_predict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZbObEk1jKyAP"},"source":["data_final2['Predicted'] = data_final2['Predicted'].str.replace('[','')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EyZLuIbYHpZi"},"source":["data_final.to_csv('EN_DATA.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yI9LiD8-IAqw"},"source":["data_final3=pd.read_csv('/content/drive/MyDrive/고려대빅데이터융합학과/빅데이터처리를위한자연어처리/EN_DATA_TEST1 - EN_DATA_TEST1.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DfpPbgyrHhoC"},"source":["data_final3['Predicted2'] = le.inverse_transform(data_final3['Predicted'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jam113OnHH38"},"source":["data_final3.to_csv('/content/drive/MyDrive/고려대빅데이터융합학과/빅데이터처리를위한자연어처리/EN_DATA_TEST2.csv',sep=',')"],"execution_count":null,"outputs":[]}]}